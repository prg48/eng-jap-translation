{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the website\n",
    "!wget \"https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dataset\n",
    "data_file = open(\"en-ja/en-ja.bicleaner05.txt\")\n",
    "data = data_file.readline().strip().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the language data (For JParaCrawl version 2)\n",
    "import pandas as pd\n",
    "lang_data = []\n",
    "\n",
    "for line in data_file:\n",
    "  data = line.strip().split(\"\\t\")\n",
    "  lang_data.append(data) \n",
    "\n",
    "lang_data_df = pd.DataFrame(lang_data, columns=[\n",
    "    \"source\", \"source_one\", \"score\", \"en\", \"jp\"\n",
    "])\n",
    "lang_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For huge files divide the files up into multiple  (For JParaCrawl version 3)\n",
    "import pandas as pd\n",
    "pre = \"drive/MyDrive/Language_translation/data\"\n",
    "filename = \"/en-jp-v3.0.\"\n",
    "chunks = []\n",
    "counter = 0\n",
    "file_counter = 1\n",
    "for line in data_file:\n",
    "  data = line.strip().split(\"\\t\")\n",
    "  chunks.append(data)\n",
    "  counter += 1\n",
    "  if counter == 5000000:\n",
    "    df = pd.DataFrame(\n",
    "        chunks,\n",
    "        columns=[\"source1\",\"source\",\"score\",\"en\",\"jp\"]\n",
    "    )\n",
    "    df.drop(\"source1\", axis=1, inplace=True)\n",
    "    create_a_dataset_and_save(df, pre + filename + str(file_counter))\n",
    "    file_counter += 1\n",
    "    counter = 0\n",
    "    chunks = []\n",
    "\n",
    "if chunks:\n",
    "  df = pd.DataFrame(\n",
    "        chunks,\n",
    "        columns=[\"source1\",\"source\",\"score\",\"en\",\"jp\"]\n",
    "    )\n",
    "  df.drop(\"source1\", axis=1, inplace=True)\n",
    "  create_a_dataset_and_save(df, pre + filename + str(file_counter))\n",
    "  counter = 0\n",
    "  file_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dataset from dataframe and save it\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_a_dataset_and_save(df, save_location):\n",
    "  lang_dataset = Dataset.from_pandas(\n",
    "      df,\n",
    "      split=\"train\"\n",
    "  )\n",
    "  lang_dataset.save_to_disk(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to google drive\n",
    "pre = \"drive/MyDrive/Language_translation/data\"\n",
    "filename = \"/en-jp-v3.0\"\n",
    "print(pre+filename)\n",
    "lang_dataset.save_to_disk(pre+filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
