{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load created datasets (This is for JParaCrawl version 3.0)\n",
    "from datasets import load_from_disk\n",
    "\n",
    "pre = \"drive/MyDrive/Language_translation/data/\"\n",
    "filename_1 = \"en-jp-v3.0.1\"\n",
    "filename_2 = \"en-jp-v3.0.2\"\n",
    "filename_3 = \"en-jp-v3.0.3\"\n",
    "filename_4 = \"en-jp-v3.0.4\"\n",
    "filename_5 = \"en-jp-v3.0.5\"\n",
    "dataset_1 = load_from_disk(pre + filename_1)\n",
    "dataset_2 = load_from_disk(pre + filename_2)\n",
    "dataset_3 = load_from_disk(pre + filename_3)\n",
    "dataset_4 = load_from_disk(pre + filename_4)\n",
    "dataset_5 = load_from_disk(pre + filename_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is only for JParaCrawl version 3.0\n",
    "# concatenate all the datasets\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# datasets lists\n",
    "datasets_list = [dataset_1, dataset_2, dataset_3, dataset_4, dataset_5]\n",
    "\n",
    "# concatenate the datasets\n",
    "combined_dataset = concatenate_datasets(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean for final dataset\n",
    "# function to split data 80% train, 10% validation, 10% test\n",
    "def create_splits(data):\n",
    "  first_split = data.train_test_split(test_size=0.20)\n",
    "  train = first_split.pop(\"train\")\n",
    "  second_split = first_split[\"test\"].train_test_split(test_size=0.50)\n",
    "  validation = second_split.pop(\"train\")\n",
    "  test = second_split.pop(\"test\")\n",
    "  second_split[\"train\"] = train\n",
    "  second_split[\"validation\"] = validation\n",
    "  second_split[\"test\"] = test\n",
    "  return second_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data\n",
    "data = combined_dataset.filter(lambda x: float(x['score']) > 0.77)\n",
    "\n",
    "# shuffle data\n",
    "data = data.shuffle()\n",
    "\n",
    "# remove source and score columns\n",
    "data = data.remove_columns([\"source\", \"score\"])\n",
    "\n",
    "# split the dataset into train, validation and test datasetdict\n",
    "data_split_1 = create_splits(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # nested function to create a translation column for datasetdict\n",
    "def create_translation_column(example):\n",
    "  example[\"translation\"] = {'en': example[\"en\"], 'jp': example[\"jp\"]}\n",
    "  return example\n",
    "\n",
    "# create the translation\n",
    "data_with_trans = data_split_1.map(create_translation_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 'en' and 'jp' columns\n",
    "final_data = data_with_trans.remove_columns([\"en\", \"jp\"])\n",
    "\n",
    "# save to disk\n",
    "final_data.save_to_disk(\"drive/MyDrive/Language_tranlsation/data/en-jp-v3.0-subset-sc-ov-77\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
