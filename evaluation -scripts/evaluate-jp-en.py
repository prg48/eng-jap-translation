from datasets import load_from_disk, load_metric
import numpy as np
from datetime import datetime
from tqdm import tqdm
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
import tensorflow as tf

# load data
main_data = load_from_disk("data/en-jp-v3.0-subset-sc-ov-77")

# remove train and test data for training. We only require validation
main_data.pop("test")
main_data.pop("train")

# load tokenizer, model,  and data_collator
checkpoint  = "Helsinki-NLP/opus-mt-jap-en"
tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors="pt", private=True, access_token=access_token)
model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)

# function to batch tokenize the data
def process_fn(batch):
  inputs = [x["jp"] for x in batch["translation"]]
  targets = [x["en"] for x in batch["translation"]]
  return tokenizer(inputs, text_target=targets, max_length=128, truncation=True)

# batch tokenize the data
tokenized_datasets = main_data.map(
    process_fn,
    batched=True,
    remove_columns=main_data["validation"].column_names
) 

metric_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

# @tf.function(jit_compile=True) decorator compiles function using XLA(Accelerated linear algebra) to improve efficiency during execution
@tf.function(jit_compile=True)
def generate_prediction_ids(batch):
  return model.generate(
      input_ids=batch["input_ids"],
      attention_mask=batch["attention_mask"],
      max_new_tokens=128,
  )

bleu = load_metric("sacrebleu")
bert = load_metric("bertscore")

def compute_metrics():
  # prepare data for metrics computation
  eval_data = model.prepare_tf_dataset(
      tokenized_datasets["validation"],
      collate_fn=metric_data_collator,
      shuffle=False,
      batch_size=64
  )

  # lists for predictions and references
  all_predictions = []
  all_references = []

  # tqdm to see progress
  for data, labels in tqdm(eval_data):
    # generate predictions ids
    predictions_ids = generate_prediction_ids(data)

    # generate target sentences from prediction ids skipping padding ids
    predictions = tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)

    ###### check if any prediction in batch is empty string
    if '' in predictions:
      print("Empty prediction in predictions")

    # strip each sentence from predictions and add it to all predictions
    all_predictions.extend(pred.strip() for pred in predictions)

    # replace -100 ids generated by data collator for masking labels with padding ids
    labels = labels.numpy()
    labels_ids = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # generate target senetences from label ids skipping padding ids
    targets = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    ###### check if any target in batch is empty string
    if '' in targets:
      print("Empty target in targets")

    # strip each sentence from targets and add it to all references. each target should be a list
    all_references.extend([targ.strip()] for targ in targets)
  
  # if all predictions or all references is empty
  if len(all_predictions) == 0:
    print("Predictions is empty")
  
  elif len(all_references) == 0:
    print("References is empty")

  # evaluate bleu and bert score for all predictions and labels 
  bleu_score = bleu.compute(predictions=all_predictions, references=all_references)
  bert_score = bert.compute(predictions=all_predictions, references=all_references, lang="en")
  return {"bleu score": bleu_score["score"], "bert_score": np.mean(bert_score["f1"])}

def writeResults(metric, fileName):
  f = open(fileName, "w")
  result = repr(metric)
  f.write("Result recorded at: {}".format(datetime.now()))
  f.write("\n")
  f.write(result)

# compute metrics and write results to file
result = compute_metrics()
writeResults(result, "drive/MyDrive/Language_translation/data/jap-en-marian-base-data-v3.0-subset-ov-77.txt")